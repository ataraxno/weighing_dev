{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import tensorflow_addons as tfa\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PATH = './models/ResNet.h5'\n",
    "TRAINING_EPOCHS = 200\n",
    "LEARNING_RATE = 0.002\n",
    "EPSILON = 1e-06\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.load('./results/2020_S/fw_dataset.npz', allow_pickle=True)\n",
    "data_indices = l['data_indices']\n",
    "input_data = l['input_data']\n",
    "output_label = l['output_label']\n",
    "INPUT_MAXS = l['INPUT_MAXS']\n",
    "INPUT_MINS = l['INPUT_MINS']\n",
    "OUTPUT_MAX = l['OUTPUT_MAX']\n",
    "OUTPUT_MIN = l['OUTPUT_MIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.astype('float32')\n",
    "output_label = output_label.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data.shape)\n",
    "print(output_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT_MAXS)\n",
    "print(INPUT_MINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OUTPUT_MAX)\n",
    "print(OUTPUT_MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_indices, input_data, output_label = shuffle(data_indices, input_data, output_label, random_state=3101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = int(input_data.shape[0]*.8)\n",
    "train_input = input_data[:N_TRAIN, ...]\n",
    "train_label = output_label[:N_TRAIN, ...]\n",
    "val_input = input_data[N_TRAIN:, ...]\n",
    "val_label = output_label[N_TRAIN:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of training set: {train_input.shape[0]}')\n",
    "print(f'number of validation set: {val_input.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    train_dataset = train_dataset.cache().shuffle(BATCH_SIZE*10).batch(BATCH_SIZE, drop_remainder=False)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_input, val_label))\n",
    "    val_dataset = val_dataset.cache().shuffle(BATCH_SIZE*10).batch(BATCH_SIZE, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.n = [128, 128, 256, 256, 512] # number of nodes\n",
    "        self.k = [1, 5, 10, 20, 50] # kernal size\n",
    "        self.s = 2 # stride (= pooling size)\n",
    "                \n",
    "        self.conv1_1 = layers.Conv1D(self.n[0]/4, self.k[0], kernel_initializer='glorot_normal', padding='same')\n",
    "        self.conv1_2 = layers.Conv1D(self.n[0]/4, self.k[1], kernel_initializer='glorot_normal', padding='same')\n",
    "        self.conv1_3 = layers.Conv1D(self.n[0]/4, self.k[2], kernel_initializer='glorot_normal', padding='same')\n",
    "        self.conv1_4 = layers.Conv1D(self.n[0]/4, self.k[3], kernel_initializer='glorot_normal', padding='same')\n",
    "        self.conv1_5 = layers.Conv1D(self.n[0]/4, self.k[4], kernel_initializer='glorot_normal', padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.pool1 = layers.MaxPooling1D(2)\n",
    "        \n",
    "        self.conv2 = layers.Conv1D(self.n[1], 1, kernel_initializer='glorot_normal', padding='valid')\n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.pool2 = layers.MaxPooling1D(2)\n",
    "        \n",
    "        self.conv3 = layers.Conv1D(self.n[2], 1, kernel_initializer='glorot_normal', padding='valid')\n",
    "        self.batch3 = layers.BatchNormalization()\n",
    "        self.activation3 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.pool3 = layers.MaxPooling1D(2)\n",
    "                                          \n",
    "        self.conv4 = layers.Conv1D(self.n[3], 1, kernel_initializer='glorot_normal', padding='valid')\n",
    "        self.batch4 = layers.BatchNormalization()\n",
    "        self.activation4 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.pool4 = layers.MaxPooling1D(2)\n",
    "        \n",
    "        self.output_conv = layers.Conv1D(self.n[4], 1, kernel_initializer='glorot_normal', padding='valid')\n",
    "        self.gate = layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = tf.concat([self.conv1_1(inp), self.conv1_2(inp), self.conv1_3(inp), self.conv1_4(inp), self.conv1_5(inp)], -1)\n",
    "        inp = self.pool1(self.activation1(self.batch1(inp)))\n",
    "        \n",
    "        inp = self.pool2(self.activation2(self.batch2(self.conv2(inp))))\n",
    "        inp = self.pool3(self.activation3(self.batch3(self.conv3(inp))))\n",
    "        inp = self.pool4(self.activation4(self.batch4(self.conv4(inp))))\n",
    "        \n",
    "        inp = self.gate(self.output_conv(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=.5, patience=2, verbose=0, mode='min',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    BEST_PATH, monitor='val_loss', verbose=0,\n",
    "    save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=EPSILON)\n",
    "    model = ResNet1D()\n",
    "    model.compile(optimizer=opt, loss='mae')\n",
    "    model.fit(train_dataset, epochs=TRAINING_EPOCHS, validation_data=val_dataset,\n",
    "                  verbose=1, callbacks=[callbacks, save, early_stop]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(BEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
