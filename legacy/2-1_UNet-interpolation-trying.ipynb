{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import (LinearLocator, MultipleLocator, FormatStrFormatter)\n",
    "from matplotlib.dates import MONDAY\n",
    "from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = ((8/2.54), (6/2.54))\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "MARKER_SIZE = 15\n",
    "cmap_m = [\"#f4a6ad\", \"#f6957e\", \"#fccfa2\", \"#8de7be\", \"#86d6f2\", \"#24a9e4\", \"#b586e0\", \"#d7f293\"]\n",
    "cmap = [\"#e94d5b\", \"#ef4d28\", \"#f9a54f\", \"#25b575\", \"#1bb1e7\", \"#1477a2\", \"#a662e5\", \"#c2f442\"]\n",
    "\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "# plt.rcParams['axes.edgecolor'] = \n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_INTV = '10 min'\n",
    "target_df = pd.read_csv('./results/SW2_greenhouse.csv', index_col='Unnamed: 0')\n",
    "target_df = target_df[['temp', 'hum', 'rad', 'loadcell_1', 'loadcell_2']]\n",
    "target_df.index = pd.DatetimeIndex(target_df.index)\n",
    "target_df = target_df.resample(DATA_INTV).first()\n",
    "target_df = target_df.interpolate(limit=35)\n",
    "target_df = target_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_p = target_df.index[0] - pd.Timedelta(DATA_INTV)\n",
    "index_start = target_df.index[0]\n",
    "data_indices = []\n",
    "for _ in target_df.index:\n",
    "    if (_ - _p) != pd.Timedelta(DATA_INTV):\n",
    "        data_indices.append([index_start, _p])\n",
    "        index_start = _\n",
    "        print(_ - _p)\n",
    "    _p = _\n",
    "data_indices.append([index_start, _p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_indices = [\n",
    "    [pd.Timestamp('2020-03-05 00:00:00'), pd.Timestamp('2020-04-20 16:47:00')],\n",
    "    [pd.Timestamp('2020-06-01 00:00:00'), pd.Timestamp('2020-06-15 23:59:00')],\n",
    "    [pd.Timestamp('2020-05-16 00:00:00'), pd.Timestamp('2020-05-31 23:59:00')],\n",
    "    [pd.Timestamp('2020-05-01 00:00:00'), pd.Timestamp('2020-05-15 23:59:00')],\n",
    "    [pd.Timestamp('2020-04-23 00:00:00'), pd.Timestamp('2020-04-30 23:59:00')],\n",
    "    [pd.Timestamp('2020-06-16 00:00:00'), pd.Timestamp('2020-07-03 23:59:00')]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=((8/2.54*6.2), (6/2.54*3.2)))\n",
    "# ax0 = plt.subplot()\n",
    "\n",
    "# ax0.spines['right'].set_visible(False)\n",
    "# ax0.spines['left'].set_position(('outward', 5))\n",
    "# ax0.spines['bottom'].set_position(('outward', 5))\n",
    "\n",
    "# ax0.plot(target_df.index, target_df['loadcell_1']/4, c=cmap[3])\n",
    "# ax0.plot(target_df.index, target_df['loadcell_2']/4, c=cmap[0])\n",
    "# ax0.plot(target_df.index, target_df['loadcell_3']/4, c=cmap[4])\n",
    "\n",
    "# ax0.set_xbound(target_df.index.min(), target_df.index.max())\n",
    "# # ax0.set_xbound(pd.Timestamp('2020-04-20 16:15:00') + pd.Timedelta('1h 24 min'), pd.Timestamp('2020-04-20 17:43:00') + pd.Timedelta('1h 24 min'))\n",
    "# ax0.xaxis.set_major_locator(LinearLocator(20))\n",
    "# ax0.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "\n",
    "# ax0.yaxis.set_major_locator(LinearLocator(5))\n",
    "# ax0.yaxis.set_minor_locator(LinearLocator(13))\n",
    "# ax0.set_ybound(2, 5)\n",
    "\n",
    "# ax0.set_xlabel('Date')\n",
    "# ax0.set_ylabel('Weight (kg)')\n",
    "\n",
    "# ax0.set_rasterized(True)\n",
    "# fig.tight_layout()\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXS = target_df.max().values\n",
    "MINS = target_df.min().values\n",
    "SCREEN_SIZE = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1e-05\n",
    "BEST_PATH = './checkpoints/UNet_loadcell_%d' % SCREEN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_indices = shuffle(data_indices, random_state=930217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_current = []\n",
    "temp_prev = []\n",
    "temp_next = []\n",
    "temp_mask = []\n",
    "temp_label = []\n",
    "for _ in data_indices[:3]:\n",
    "    print(_)\n",
    "    temp_df = target_df[_[0]:_[1]].values\n",
    "    if temp_df.shape[0] < SCREEN_SIZE*3:\n",
    "        print('Shorter than the screen size.')\n",
    "        continue\n",
    "    np.random.seed(3101)\n",
    "    null_prob = 0.2\n",
    "    mask = np.random.choice(2, temp_df.size, p=[null_prob, 1-null_prob]).reshape(temp_df.shape)\n",
    "    np.random.seed(4564)\n",
    "    raw_null_prob = 0.3\n",
    "    null_len = 60\n",
    "    raw_indices = np.random.choice(np.arange(int(temp_df.shape[0]/null_len)-1),\n",
    "                                   replace=False, size=int(temp_df.shape[0]/null_len * raw_null_prob))\n",
    "    raw_indices = raw_indices*null_len\n",
    "    raw_indices_ext = []\n",
    "    if len(raw_indices) == 0:\n",
    "        print('Too short.')\n",
    "        continue\n",
    "    for elem in raw_indices:\n",
    "        for _ in range(elem, elem+null_len):\n",
    "            raw_indices_ext.append(_)\n",
    "    raw_indices = np.array(raw_indices_ext)\n",
    "    raw_indices = np.unique(raw_indices[raw_indices < temp_df.shape[0]])\n",
    "    temp_df = (temp_df - MINS)/(MAXS - MINS)\n",
    "    \n",
    "    mask[raw_indices, :] = 0\n",
    "    missing_df = np.ma.array(temp_df, mask=1-mask, fill_value=-1)\n",
    "    missing_df = missing_df.filled()\n",
    "    for INDEX in range(SCREEN_SIZE*2, temp_df.shape[0]-SCREEN_SIZE):\n",
    "        temp_prev.append(missing_df[(INDEX-SCREEN_SIZE*2):(INDEX-SCREEN_SIZE), :])\n",
    "        temp_current.append(missing_df[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "        temp_next.append(missing_df[INDEX:INDEX+SCREEN_SIZE, :])\n",
    "        \n",
    "        temp_mask.append(mask[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "        temp_label.append(temp_df[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "temp_prev = np.stack(temp_prev).astype(np.float32)\n",
    "temp_current = np.stack(temp_current).astype(np.float32)\n",
    "temp_next = np.stack(temp_next).astype(np.float32)\n",
    "temp_mask = np.stack(temp_mask).astype(np.float32)\n",
    "temp_label = np.stack(temp_label).astype(np.float32)\n",
    "\n",
    "raw_input = np.stack([temp_current, temp_mask, temp_prev, temp_next], axis = -1)\n",
    "raw_label = temp_label[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(temp_current)\n",
    "del(temp_prev)\n",
    "del(temp_next)\n",
    "del(temp_mask)\n",
    "del(temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = np.concatenate([raw_input]*(int(SCREEN_SIZE/target_df.shape[1])), axis=2)\n",
    "raw_label = np.concatenate([raw_label]*(int(SCREEN_SIZE/target_df.shape[1])), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_current = []\n",
    "temp_prev = []\n",
    "temp_next = []\n",
    "temp_mask = []\n",
    "temp_label = []\n",
    "for _ in data_indices[3:]:\n",
    "    print(_)\n",
    "    temp_df = target_df[_[0]:_[1]].values\n",
    "    if temp_df.shape[0] < SCREEN_SIZE*3:\n",
    "        print('Shorter than the screen size.')\n",
    "        continue\n",
    "    np.random.seed(3101)\n",
    "    null_prob = 0.2\n",
    "    mask = np.random.choice(2, temp_df.size, p=[null_prob, 1-null_prob]).reshape(temp_df.shape)\n",
    "    np.random.seed(4564)\n",
    "    raw_null_prob = 0.3\n",
    "    null_len = 60\n",
    "    raw_indices = np.random.choice(np.arange(int(temp_df.shape[0]/null_len)-1),\n",
    "                                   replace=False, size=int(temp_df.shape[0]/null_len * raw_null_prob))\n",
    "    raw_indices = raw_indices*null_len\n",
    "    raw_indices_ext = []\n",
    "    if len(raw_indices) == 0:\n",
    "        print('Too short.')\n",
    "        continue\n",
    "    for elem in raw_indices:\n",
    "        for _ in range(elem, elem+null_len):\n",
    "            raw_indices_ext.append(_)\n",
    "    raw_indices = np.array(raw_indices_ext)\n",
    "    raw_indices = np.unique(raw_indices[raw_indices < temp_df.shape[0]])\n",
    "    temp_df = (temp_df - MINS)/(MAXS - MINS)\n",
    "    \n",
    "    mask[raw_indices, :] = 0\n",
    "    missing_df = np.ma.array(temp_df, mask=1-mask, fill_value=-1)\n",
    "    missing_df = missing_df.filled()\n",
    "    for INDEX in range(SCREEN_SIZE*2, temp_df.shape[0]-SCREEN_SIZE):\n",
    "        temp_prev.append(missing_df[(INDEX-SCREEN_SIZE*2):(INDEX-SCREEN_SIZE), :])\n",
    "        temp_current.append(missing_df[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "        temp_next.append(missing_df[INDEX:INDEX+SCREEN_SIZE, :])\n",
    "        \n",
    "        temp_mask.append(mask[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "        temp_label.append(temp_df[(INDEX-SCREEN_SIZE):INDEX, :])\n",
    "temp_prev = np.stack(temp_prev).astype(np.float32)\n",
    "temp_current = np.stack(temp_current).astype(np.float32)\n",
    "temp_next = np.stack(temp_next).astype(np.float32)\n",
    "temp_mask = np.stack(temp_mask).astype(np.float32)\n",
    "temp_label = np.stack(temp_label).astype(np.float32)\n",
    "\n",
    "test_input = np.stack([temp_current, temp_mask, temp_prev, temp_next], axis = -1)\n",
    "test_label = temp_label[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.concatenate([test_input]*(int(SCREEN_SIZE/target_df.shape[1])), axis=2)\n",
    "test_label = np.concatenate([test_label]*(int(SCREEN_SIZE/target_df.shape[1])), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(temp_current)\n",
    "del(temp_prev)\n",
    "del(temp_next)\n",
    "del(temp_mask)\n",
    "del(temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_input.shape)\n",
    "print(raw_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_input.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(raw_input[i, ..., 2])\n",
    "print(raw_input[i, ..., 0])\n",
    "print(raw_input[i, ..., 3])\n",
    "print(raw_input[i, ..., 1])\n",
    "print(raw_label[i, ..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAXS)\n",
    "print(MINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_input = raw_input.astype(np.float32)\n",
    "# raw_label = raw_label.astype(np.float32)\n",
    "# test_input = test_input.astype(np.float32)\n",
    "# test_label = test_label.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input, raw_label = shuffle(raw_input, raw_label, random_state=4574)\n",
    "test_input, test_label = shuffle(test_input, test_label, random_state=3110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((raw_input, raw_label))\n",
    "train_dataset = train_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "val_dataset = val_dataset.cache().shuffle(BATCH_SIZE*50).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(self.filters, self.kernel_size, kernel_initializer='he_normal', padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.filters, self.kernel_size, kernel_initializer='he_normal', padding='same')\n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        inp = self.activation1(self.batch1(self.conv1(inp), training=TRAINING))\n",
    "        inp = self.activation2(self.batch2(self.conv2(inp), training=TRAINING))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.filters, self.kernel_size, kernel_initializer='he_normal',\n",
    "                                              strides=self.strides, padding='same')\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.activation1(self.deconv1(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.conv_block1 = ConvBlock(64, (2, 2))\n",
    "        self.pool1 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(128, (2, 2))\n",
    "        self.pool2 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(256, (2, 2))\n",
    "        self.pool3 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(512, (2, 2))\n",
    "        self.pool4 = layers.MaxPooling2D()\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(1024, (2, 2))\n",
    "        \n",
    "        self.deconv_block1 = DeconvBlock(512, (2, 2), (2, 2))\n",
    "        self.conv_block_r1 = ConvBlock(512, (2, 2))\n",
    "        \n",
    "        self.deconv_block2 = DeconvBlock(256, (2, 2), (2, 2))\n",
    "        self.conv_block_r2 = ConvBlock(256, (2, 2))\n",
    "        \n",
    "        self.deconv_block3 = DeconvBlock(128, (2, 2), (2, 2))\n",
    "        self.conv_block_r3 = ConvBlock(128, (2, 2))\n",
    "        \n",
    "        self.deconv_block4 = DeconvBlock(64, (2, 2), (2, 2))\n",
    "        self.conv_block_r4 = ConvBlock(64, (2, 2))\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=False):\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        \n",
    "        deconv1 = self.deconv_block1(bottom)\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        deconv3 = self.padding(self.deconv_block3(deconv2))\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss inputs should be masked.\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "def loss_function(model, inp, tar, TRAINING=False):\n",
    "    \n",
    "    masked_real = tar * (1 - inp[..., 1:2])\n",
    "    masked_pred = model(inp, TRAINING) * (1 - inp[..., 1:2])\n",
    "    \n",
    "    return loss_object(masked_real, masked_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# def loss_function(model, inp, tar, TRAINING=False):\n",
    "    \n",
    "#     masked_real = tar\n",
    "#     masked_pred = model(inp, TRAINING)\n",
    "    \n",
    "#     return loss_object(masked_real, masked_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNet()\n",
    "opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(loss_function, model, opt, inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients = tape.gradient(loss_function(model, inp, tar, True), model.trainable_variables)\n",
    "        gradient_variables = zip(gradients, model.trainable_variables)\n",
    "        opt.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = BEST_PATH\n",
    "\n",
    "ckpt = tf.train.Checkpoint(unet_model=unet_model,\n",
    "                           opt=opt)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_test_loss = 100.0\n",
    "early_stop_buffer = 100\n",
    "DISP_STEPS = 100\n",
    "with writer.as_default():\n",
    "    with tf.summary.record_if(True):\n",
    "        for epoch in range(500):\n",
    "            for step, (inp, tar) in enumerate(train_dataset):\n",
    "                train(loss_function, unet_model, opt, inp, tar)\n",
    "                loss_values = loss_function(unet_model, inp, tar)\n",
    "                tf.summary.scalar('loss', loss_values, step=step)\n",
    "                \n",
    "                if step % DISP_STEPS == 0:\n",
    "                    test_loss = 0\n",
    "                    for step_, (inp_, tar_) in enumerate(val_dataset):\n",
    "                        test_loss += loss_function(unet_model, inp_, tar_)\n",
    "                        \n",
    "                        if step_ > DISP_STEPS:\n",
    "                            test_loss /= DISP_STEPS\n",
    "                            break\n",
    "                    if test_loss.numpy() < prev_test_loss:\n",
    "                        ckpt_save_path = ckpt_manager.save()\n",
    "                        prev_test_loss = test_loss.numpy()\n",
    "                        print('Saving checkpoint at {}'.format(ckpt_save_path))\n",
    "                        early_stop_buffer = 100\n",
    "                    else:\n",
    "                        early_stop_buffer -= 1\n",
    "                    print('Epoch {} batch {} train loss: {:.4f} test loss: {:.4f}'\n",
    "                          .format(epoch, step, loss_values.numpy(), test_loss.numpy()))\n",
    "                if early_stop_buffer <= 0:\n",
    "                    print('early stop.')\n",
    "                    break\n",
    "            if early_stop_buffer <= 0:\n",
    "                    break                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "if ckpt_manager.checkpoints:\n",
    "    ckpt.restore(ckpt_manager.checkpoints[i])\n",
    "    print ('Checkpoint ' + ckpt_manager.checkpoints[i][-2:] +' restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = unet_model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pred = []\n",
    "OUTLIER = 3\n",
    "for __ in range(target_df.shape[1]):\n",
    "    temp = []\n",
    "    for _ in range(int(pred_result.shape[1]/target_df.shape[1])):\n",
    "        temp.append(pred_result[..., _*target_df.shape[1]:(_+1)*target_df.shape[1], 0][..., __])\n",
    "    temp = np.stack(temp, axis=2)\n",
    "    temp.sort(axis=2)\n",
    "    avg_pred.append(temp[..., OUTLIER:-OUTLIER].mean(axis=2))\n",
    "avg_pred = np.stack(avg_pred, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking = test_input[..., 1]\n",
    "avg_masking = masking[..., :target_df.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pred = np.ma.array(pred_result[..., 0], mask=masking)\n",
    "masked_avg_pred = np.ma.array(avg_pred, mask=avg_masking)\n",
    "masked_label = np.ma.array(test_label[..., 0], mask=masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = np.arange(0, test_label.shape[1])\n",
    "for _ in range (2):\n",
    "\n",
    "        NUMBERS = np.arange(1, pred_result.shape[0])\n",
    "        np.random.shuffle(NUMBERS)\n",
    "        NUMBERS = NUMBERS[:6]\n",
    "        position = 331\n",
    "        fig = plt.figure(figsize=((8.5/2.54)*8, (6/2.54)*8))\n",
    "        \n",
    "        i=-1\n",
    "        for NUMBER in NUMBERS:\n",
    "            ax = plt.subplot(position)\n",
    "            measured1 = plt.plot(x_t, test_label[NUMBER, :, i], c='k', alpha=1) #measured\n",
    "            expect1 = plt.plot(x_t, masked_pred[NUMBER, :, i], 'o', c=cmap[5], alpha=1) #estimated\n",
    "            expect1 = plt.plot(x_t, masked_avg_pred[NUMBER, :, i], 'o', c=cmap[0], alpha=1) #estimated\n",
    "            expect2 = plt.plot(x_t, pred_result[NUMBER, :, i], c=cmap[2], alpha=1) #estimated\n",
    "            ax.axis('off')\n",
    "\n",
    "            position += 1\n",
    "        plt.show()\n",
    "        _ += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
